# -*- coding: utf-8 -*-
"""Electric Power Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kxGOKqbgqh_T-9DNDLSmYfaRrt04Chgq
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %matplotlib inline
import seaborn as sns
import numpy as np
import os
import matplotlib.pyplot as plt
import plotly
import plotly.graph_objects as go
from google.colab import files
from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
import os
# !pip install jupyter-dash
import dash
import torch 
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
import math
from sklearn.svm import SVR
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import sklearn.neural_network as ml
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from dash.dependencies import Input, Output, State, ClientsideFunction
import dash_core_components as dcc
import dash_html_components as html

uploaded = files.upload()

"""**Basic Data Preparation**"""

#Load Data
df=pd.read_excel("data/Folds5x2_pp.xlsx", sep=',', error_bad_lines=False, index_col=False, dtype='unicode')

df=df.apply(pd.to_numeric)#Transfer to Numeric
df.isnull().sum()

df.describe()#Basic description of the data

df.cov() #Covarriance of the dataset

df.corr() #Correlation between the features

pd.plotting.scatter_matrix(df,diagonal="kde",figsize=(20,20))#Scatter Matrix to find any pattern
plt.savefig(r'Scatter Matrix.png')
files.download('Scatter Matrix.png')
#Heatmap

corr = df.corr()
plt.figure(figsize=(30,10))
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)

ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
);
plt.title("Correlation Heatmap of the dataset")
# plt.savefig(r'heatmap.png')
# files.download('heatmap.png')

X=df.loc[:,"AT":"RH"]#Input Features
y=df["PE"]#Output Feature
X_train, X_test, y_train, y_test= train_test_split(X,y,train_size=0.8,random_state=0) #80% training data and 20% testing data for whole dataset without any Standardization or Normalizations

print(type(min(X.min(axis=0))))

"""**Standardization.**"""

X_scale_val=df.loc[:,"AT":"RH"].values
y_scale_val=df["PE"].values
X_scale=StandardScaler()#Standerdizing the data.
y_scale=StandardScaler()
X_after_scale=X_scale.fit_transform(X_scale_val)
y_after_scale=y_scale.fit_transform(y_scale_val.reshape(-1,1))
X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled= train_test_split(X_after_scale,y_after_scale,train_size=0.8,random_state=0)#Standardized Train test Spilit

"""**Normalization.**"""

X_norm=MinMaxScaler()#Normalizing data.
y_norm=MinMaxScaler()
X_after_norm=X_norm.fit_transform(X)
y_after_norm=y_norm.fit_transform(y.values.reshape(-1,1))
X_train_norm, X_test_norm, y_train_norm, y_test_norm= train_test_split(X_after_norm,y_after_norm,train_size=0.8, random_state=0)#Normalized Train test Spilit

"""**MAE(Mean absolute error) , RMSE (Root Mean Squared Error) ,R^2 (Coefficient of determination) for Linear Regression.**"""

#Linear Regression Train
lr=LinearRegression()
lr.fit(X_train_scaled,y_train_scaled)

lr.coef_

lr.intercept_

#Linear Regression Prediction
Y_lr_predict=lr.predict(X_test_scaled)

#MAE
mean_absolute_error(y_test_scaled,Y_lr_predict)

#RMSE
math.sqrt(mean_squared_error(y_test_scaled, Y_lr_predict))

#R^2 score
lr.score(X_test_scaled,y_test_scaled)

#R^2 score of Linear Regression after K-fold cross validation
lr_for_cross_validation=LinearRegression()
cross_validation_lr=cross_val_score(lr_for_cross_validation,X_after_scale,y_after_scale,cv=10)
cross_validation_lr.mean()

#MAE score of Linear Regression after K-fold cross validation 
cross_validation_lr=cross_val_score(lr_for_cross_validation,X_after_scale,y_after_scale,cv=10,scoring='neg_mean_absolute_error')
cross_validation_lr=-cross_validation_lr
cross_validation_lr.mean()

#RMSE score of Linear Regression after K-fold cross validation 
cross_validation_lr=cross_val_score(lr_for_cross_validation,X_after_scale,y_after_scale,cv=10,scoring='neg_mean_squared_error')
cross_validation_lr=-cross_validation_lr
cross_validation_lr=cross_validation_lr.mean()
math.sqrt(cross_validation_lr)

plt.figure(figsize=[25,10])
sns.regplot(x=y_test_scaled, y=Y_lr_predict, scatter_kws = {'color': 'g','alpha': 0.3}, line_kws = {'color': 'red','alpha': 0.8})
plt.title("Figure for Linear Regression with testing and predicted data with regression line", fontsize=25)
plt.xlabel("Test data \n\nMean Absolute Error: 0.21257705181382253 \n Root Mean Squared Error: 0.26719809083015766",fontsize=15)
plt.ylabel("Predicted Value",fontsize=15)
# plt.legend(['Training','Predicted'])
plt.savefig("Linear Regression.png",format="png")
files.download("Linear Regression.png")

"""**MAE(Mean absolute error) , RMSE (Root Mean Squared Error) ,R^2 (Coefficient of determination) for Support Vector Machine.**"""

svr=SVR()

#Hyperparameter Tuning
param_for_SVR={
    'kernel':['linear', 'poly', 'rbf'],
    'C':[1,10,20]
}
SVR_hyper_optimization= GridSearchCV(svr,param_grid=param_for_SVR,n_jobs=-1,cv=5,verbose=3)

SVR_hyper_optimization.fit(X_after_scale,y_after_scale.ravel())

SVR_hyper_optimization.best_estimator_

svr=SVR(C=20, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)

#SVR Train and prediction
svr.fit(X_train_scaled,y_train_scaled.ravel())
svr_predict=svr.predict(X_test_scaled)

#MAE
mean_absolute_error(y_test_scaled,svr_predict)

#RMSE
math.sqrt(mean_squared_error(y_test_scaled, svr_predict))

#R^2 score
svr.score(X_test_scaled,y_test_scaled)

#R^2 value of SVR after K-fold cross_validation
svr_for_cross_validation=SVR(C=30, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
cross_validation_svr=cross_val_score(svr_for_cross_validation,X_after_scale,y_after_scale.ravel(),cv=10)
cross_validation_svr.mean()

#MAE score of SVR after K-fold cross validation 
cross_validation_svr=cross_val_score(svr_for_cross_validation,X_after_scale,y_after_scale.ravel(),cv=10,scoring='neg_mean_absolute_error')
cross_validation_svr=-cross_validation_svr
cross_validation_svr.mean()

#RMSE score of SVR after K-fold cross validation 
cross_validation_svr=cross_val_score(svr_for_cross_validation,X_after_scale,y_after_scale.ravel(),cv=10,scoring='neg_mean_squared_error')
cross_validation_svr=-cross_validation_svr
cross_validation_svr=cross_validation_svr.mean()
math.sqrt(cross_validation_svr)

plt.figure(figsize=[25,10])

sns.regplot(x=y_test_scaled, y=svr_predict, scatter_kws = {'color': 'g','alpha': 0.3}, line_kws = {'color': 'red','alpha': 0.8})
plt.title("Figure for Support Vector Machine with training and predicted data with regression line", fontsize=25)
plt.xlabel("Test Data \n\nMean Absolute Error: 0.17189446650992435 \n Root Mean Squared Error: 0.23174338510935957",fontsize=15)
plt.ylabel("Predicted Value",fontsize=15)

# plt.legend(['Training','Predicted'])
plt.savefig("Support Vector.png",format="png")
files.download("Support Vector.png")

"""**MAE(Mean absolute error) , RMSE (Root Mean Squared Error) ,R^2 (Coefficient of determination) for Random Forest.**"""

#Hyperparameter Tuning
param_for_randomFR={
        'n_estimators':[50,100,200,300],
        'criterion':['mse', 'mae'],
        'max_features':['auto','sqrt','log2']
}

randomFR=RandomForestRegressor()
randomFR_hyper_optimization= RandomizedSearchCV(randomFR,param_distributions=param_for_randomFR,n_jobs=-1,cv=5,verbose=3)
randomFR_hyper_optimization.fit(X_train,y_train)

randomFR_hyper_optimization.best_estimator_

randomFR=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',
                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=300, n_jobs=None, oob_score=False,
                      random_state=None, verbose=0, warm_start=False)
randomFR.fit(X_train,y_train)
randomFR_predict=randomFR.predict(X_test)

#MAE
mean_absolute_error(y_test,randomFR_predict)

#RMSE
math.sqrt(mean_squared_error(y_test, randomFR_predict))

#R^2 score
randomFR.score(X_test,y_test)

#R^2 value of Random Forest after K-fold cross_validation
randomFR_for_cross_validation=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',
                      max_depth=None, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=300, n_jobs=None, oob_score=False,
                      random_state=None, verbose=0, warm_start=False)
cross_validation_randomFR=cross_val_score(randomFR_for_cross_validation,X,y,cv=10)
cross_validation_randomFR.mean()

#MAE score of Random Forest after K-fold cross validation 
cross_validation_randomFR=cross_val_score(randomFR_for_cross_validation,X,y,cv=10,scoring='neg_mean_absolute_error')
cross_validation_randomFR=-cross_validation_randomFR
cross_validation_randomFR.mean()

#RMSE score of Random Forest after K-fold cross validation 
cross_validation_randomFR=cross_val_score(svr_for_cross_validation,X,y,cv=10,scoring='neg_mean_squared_error')
cross_validation_randomFR=-cross_validation_randomFR
cross_validation_randomFR=cross_validation_randomFR.mean()
math.sqrt(cross_validation_randomFR)

plt.figure(figsize=[25,10])
sns.regplot(x=y_test, y=randomFR_predict, scatter_kws = {'color': 'g','alpha': 0.3}, line_kws = {'color': 'red','alpha': 0.8})
plt.title("Figure for Random Forest with training and predicted data with regression line", fontsize=25)
plt.xlabel("Test Data \n\nMean Absolute Error: 2.2303251924926775 \n Root Mean Squared Error: 5.298454126257121",fontsize=15)
plt.ylabel("Predicted Value",fontsize=15)
plt.savefig("Random Forest.png",format="png")
files.download("Random Forest.png")

"""**MAE(Mean absolute error) , RMSE (Root Mean Squared Error) ,R^2 (Coefficient of determination) for Extreme Gradiant Boosting.**"""

#Hyperparameter Tuning
param_for_xgboost={
    'learning_rate'   :[0.05,0.1,0.15,0.2,0.25,0.3],
    'max_depth'       :[3,4,5,10,15,20],
    'min_child_weight':[1,3,5,7],
    'gamma'           :[0,0.1,0.2,0.3,0.4],
    'booster'         :['gbtree', 'gblinear','dart'],
    'objective'       :['reg:squarederror']
}

xgboost=xgb.XGBRegressor()
xgboost_hyper_optimization=RandomizedSearchCV(xgboost,param_distributions=param_for_xgboost,n_jobs=-1,cv=5,verbose=3)

xgboost_hyper_optimization.fit(X,y)

xgboost_hyper_optimization.best_estimator_

xgboost=xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0.3,
             importance_type='gain', learning_rate=0.1, max_delta_step=0,
             max_depth=20, min_child_weight=1, missing=None, n_estimators=100,
             n_jobs=1, nthread=None, objective='reg:squarederror',
             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,
             seed=None, silent=None, subsample=1, verbosity=1)

#Extreme Gradiant Boosting Train
xgboost.fit(X_train,y_train)

#Extreme Gradiant Boosting Test
xgboost_predict=xgboost.predict(X_test)

#MAE
mean_absolute_error(y_test,xgboost_predict)

#RMSE
math.sqrt(mean_squared_error(y_test, xgboost_predict))

#R^2 score
xgboost.score(X_test,y_test)

#R^2 value of Extreme Gradiant Boosting after K-fold cross_validation
xgboost_for_cross_validation=xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, gamma=0.1,
             importance_type='gain', learning_rate=0.15, max_delta_step=0,
             max_depth=20, min_child_weight=5, missing=None, n_estimators=100,
             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
             silent=None, subsample=1, verbosity=1)
cross_validation_xgboost=cross_val_score(xgboost_for_cross_validation,X,y,cv=10)
cross_validation_xgboost.mean()

#MAE score of Extreme Gradiant Boosting after K-fold cross validation 
cross_validation_xgboost=cross_val_score(xgboost_for_cross_validation,X,y,cv=10,scoring='neg_mean_absolute_error')
cross_validation_xgboost=-cross_validation_xgboost
cross_validation_xgboost.mean()

#RMSE score of Extreme Gradiant Boosting after K-fold cross validation 
cross_validation_xgboost=cross_val_score(xgboost_for_cross_validation,X,y,cv=10,scoring='neg_mean_squared_error')
cross_validation_xgboost=-cross_validation_xgboost
cross_validation_xgboost=cross_validation_xgboost.mean()
math.sqrt(cross_validation_xgboost)

plt.figure(figsize=[25,10])
sns.regplot(x=y_test, y=xgboost_predict, scatter_kws = {'color': 'g','alpha': 0.3}, line_kws = {'color': 'red','alpha': 0.8})
plt.title("Figure for Extreme Gradiant Boosting with training and predicted data with regression line", fontsize=25)
plt.xlabel("Test Data \n\nMean Absolute Error: 2.0500352389367147 \n Root Mean Squared Error: 2.9709087128951",fontsize=15)
plt.ylabel("Predicted Value",fontsize=15)
plt.savefig("Extreme Gradiant Boosting.png",format="png")
files.download("Extreme Gradiant Boosting.png")

"""**MAE(Mean absolute error) , RMSE (Root Mean Squared Error) ,R^2 (Coefficient of determination) for Neural Network.**"""

#Hyperparameter Tuning
param_for_ann={
    'activation': ['logistic','relu'],
    'solver':['lbgfs','sgd','adam'],
    'max_iter':[200,300,400,500],
    'learning_rate':['constant','adaptive'],
    'learning_rate_init':[.001,.01,.1,.2,.3],
    'hidden_layer_sizes':[100,200,50]
}

ann=ml.MLPRegressor()
ann_hyper_optimization=RandomizedSearchCV(ann,param_distributions=param_for_ann,n_jobs=-1,cv=5,verbose=3)
ann_hyper_optimization.fit(X_train_norm,y_train_norm.ravel())

ann_hyper_optimization.best_estimator_

#Neural Network Training
ann=ml.MLPRegressor(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9,
             beta_2=0.999, early_stopping=False, epsilon=1e-08,
             hidden_layer_sizes=200, learning_rate='constant',
             learning_rate_init=0.01, max_fun=15000, max_iter=400, momentum=0.9,
             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
             random_state=None, shuffle=True, solver='adam', tol=0.0001,
             validation_fraction=0.1, verbose=False, warm_start=False)
ann.fit(X_train_norm,y_train_norm.ravel())
#Neural Network prediction
ann_predict=ann.predict(X_test_norm)

#MAE
mean_absolute_error(y_test_norm,aan_predict)

#RMSE
math.sqrt(mean_squared_error(y_test_norm, aan_predict))

#R^2 Score
ann.score(X_test_norm,y_test_norm)

#R^2 value of Neural Network after K-fold cross_validation
ann_for_cross_validation=ml.MLPRegressor(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9,
             beta_2=0.999, early_stopping=False, epsilon=1e-08,
             hidden_layer_sizes=200, learning_rate='constant',
             learning_rate_init=0.01, max_fun=15000, max_iter=400, momentum=0.9,
             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
             random_state=None, shuffle=True, solver='adam', tol=0.0001,
             validation_fraction=0.1, verbose=False, warm_start=False)
cross_validation_ann=cross_val_score(ann_for_cross_validation,X_after_norm,y_after_norm.ravel(),cv=10)
cross_validation_ann.mean()

#MAE score of Neural Network after K-foldcross validation 
cross_validation_ann=cross_val_score(ann_for_cross_validation,X_after_norm,y_after_norm.ravel(),cv=10,scoring='neg_mean_absolute_error')
cross_validation_ann=-cross_validation_ann
cross_validation_ann.mean()

#RMSE score of Extreme Neural Network after K-fold cross validation 
cross_validation_ann=cross_val_score(ann_for_cross_validation,X_after_norm,y_after_norm.ravel(),cv=10,scoring='neg_mean_squared_error')
cross_validation_ann=-cross_validation_ann
cross_validation_ann=cross_validation_ann.mean()
math.sqrt(cross_validation_ann)

plt.figure(figsize=(25,10))
sns.regplot(x=y_test_norm, y=ann_predict, scatter_kws = {'color': 'g','alpha': 0.3}, line_kws = {'color': 'red','alpha': 0.8})
plt.title("Figure for Neural Network with training and predicted data with regression line", fontsize=25)
plt.xlabel("Test Data \n\nMean Absolute Error: 0.04712749212870557 < 1 \n Root Mean Squared Error: 0.061314131512070706 < 1",fontsize=15)
plt.ylabel("Predicted Value",fontsize=15)
plt.savefig("Neural Network.png",format="png")
files.download("Neural Network.png")